# Data-Science-Fundamentals-Statistical-Analysis
This documentation aims to document and experiment with common statistical assessment strategies and their impact on Machine Learning projects. 

Some useful techniques that are discussed in python notebook are: 

**Hypothesis Testing** (using Pearson Correlation)
Hypothesis testing allows us to check the statistical validity of a hypothesis. In the ML context, when we have many features, it allows us to explore whether a given feature has a ststistically significant correlation with target variable or not. 

**Bias-Variance Analysis**
This step is useful to check model and data compatibility via bias and variance.
Bias: When model isn't able to capture the complexity of the data. Irrespective of training or testing, model has high error.
Variance: When model conplexity is too high for the data. This results in overfit, meaning low error with training but very high error with test data.

**Confidence intervals**
Confidence Interval answers that, how much would this estimate change if we repeated the experiment?

**Prediction Uncertainty**

**Metrics reliability test**
